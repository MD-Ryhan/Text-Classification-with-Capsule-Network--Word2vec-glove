{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MD-Ryhan/Text-Classification-with-Capsule-Network--Word2vec-glove/blob/main/word2vec%2BCapsule_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbdcc8e-d090-4182-994c-bbd2381956eb",
      "metadata": {
        "id": "5cbdcc8e-d090-4182-994c-bbd2381956eb",
        "outputId": "f18cc096-9163-4237-a0ce-429212a58ae8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-22 00:33:23.064431: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-22 00:33:23.065105: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-22 00:33:23.068882: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-22 00:33:24.687293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D , LSTM\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f41157-5d9a-4f3a-a095-c8fe69d9d39e",
      "metadata": {
        "id": "94f41157-5d9a-4f3a-a095-c8fe69d9d39e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dense\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.metrics import Metric\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from tensorflow.keras import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b299dd2d-c2f4-4472-b219-1d86b3add227",
      "metadata": {
        "id": "b299dd2d-c2f4-4472-b219-1d86b3add227"
      },
      "outputs": [],
      "source": [
        "# Load the training and test data from Excel files\n",
        "train_data = pd.read_excel('eng-train.xlsx')\n",
        "test_data = pd.read_excel('eng-test.xlsx')\n",
        "val_data = pd.read_excel('eng-val.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f1224c-4878-4470-a1b2-1d663d239afb",
      "metadata": {
        "id": "77f1224c-4878-4470-a1b2-1d663d239afb"
      },
      "outputs": [],
      "source": [
        "# Split the data into features (X) and labels (y)\n",
        "X_train = train_data['text'].tolist()\n",
        "y_train = train_data['label'].tolist()\n",
        "X_test = test_data['text'].tolist()\n",
        "y_test = test_data['label'].tolist()\n",
        "X_val = val_data['text'].tolist()\n",
        "y_val = val_data['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0311df5-686b-492e-98b4-fda150c86e96",
      "metadata": {
        "id": "c0311df5-686b-492e-98b4-fda150c86e96"
      },
      "outputs": [],
      "source": [
        "# Word2Vec\n",
        "# Train Word2Vec model on the training data\n",
        "sentences = [text.split() for text in X_train]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, sg=1)\n",
        "\n",
        "# Transform each text into a fixed-length vector using Word2Vec embeddings\n",
        "X_train_w2v = np.array([np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
        "                               or [np.zeros(100)], axis=0) for sentence in sentences])\n",
        "X_test_w2v = np.array([np.mean([word2vec_model.wv[word] for word in text.split() if word in word2vec_model.wv]\n",
        "                              or [np.zeros(100)], axis=0) for text in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa8f654b-3bad-4b0d-a133-d72e94f17979",
      "metadata": {
        "id": "fa8f654b-3bad-4b0d-a133-d72e94f17979"
      },
      "outputs": [],
      "source": [
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 100\n",
        "embed_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d78fe5b-9183-4c51-8a1c-2e0d2c98deaf",
      "metadata": {
        "id": "0d78fe5b-9183-4c51-8a1c-2e0d2c98deaf"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15887203-df2f-4448-83cf-d16aed639804",
      "metadata": {
        "id": "15887203-df2f-4448-83cf-d16aed639804"
      },
      "outputs": [],
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b56fd67-9b11-41cd-bcee-5ae3ad73b316",
      "metadata": {
        "id": "5b56fd67-9b11-41cd-bcee-5ae3ad73b316"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 100\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d566fb1-90e0-4ccf-9ed0-c09aac86b0c8",
      "metadata": {
        "id": "7d566fb1-90e0-4ccf-9ed0-c09aac86b0c8"
      },
      "outputs": [],
      "source": [
        "# Create a weight matrix for the embedding layer\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44371f4e-b63c-4713-a17a-60bb16701d40",
      "metadata": {
        "id": "44371f4e-b63c-4713-a17a-60bb16701d40"
      },
      "outputs": [],
      "source": [
        "training_padded = np.array(X_train)\n",
        "training_labels = np.array(y_train)\n",
        "val_padded = np.array(X_val)\n",
        "val_labels = np.array(y_val)\n",
        "testing_padded = np.array(X_test)\n",
        "testing_labels = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ab6404-043a-4d78-8f14-48338e10a332",
      "metadata": {
        "id": "31ab6404-043a-4d78-8f14-48338e10a332"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import Metric\n",
        "\n",
        "# Custom metric for F1 score\n",
        "class F1Score(Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
        "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(tf.math.round(y_pred), tf.float32)\n",
        "        self.true_positives.assign_add(tf.reduce_sum(y_true * y_pred))\n",
        "        self.false_positives.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
        "        self.false_negatives.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
        "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
        "        f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "        return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fee4db5-264c-4f8b-8b3b-b5ca721bca08",
      "metadata": {
        "id": "7fee4db5-264c-4f8b-8b3b-b5ca721bca08"
      },
      "outputs": [],
      "source": [
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "import tensorflow\n",
        "\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "\n",
        "#define our own softmax function instead of K.softmax\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "#A Capsule Implement with Pure Keras\n",
        "class Capsule(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Capsule, self).build(input_shape)\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(1, input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(input_num_capsule,\n",
        "                                            input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
        "        else:\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(u_vecs)[0]\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
        "                                            self.num_capsule, self.dim_capsule))\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "\n",
        "        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            # o = K.batch_dot(c, u_hat_vecs, [2, 2])\n",
        "            o = tensorflow.einsum('bin,binj->bij', c, u_hat_vecs)\n",
        "            if K.backend() == 'theano':\n",
        "                o = K.sum(o, axis=1)\n",
        "            if i < self.routings - 1:\n",
        "                o = K.l2_normalize(o, -1)\n",
        "                # b = K.batch_dot(o, u_hat_vecs, [2, 3])\n",
        "                b = tf.einsum('bij,binj->bin', o, u_hat_vecs)\n",
        "                if K.backend() == 'theano':\n",
        "                    b = K.sum(b, axis=1)\n",
        "\n",
        "        return self.activation(o)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb276b4-f96f-4d8d-b03c-0451269b1679",
      "metadata": {
        "id": "4eb276b4-f96f-4d8d-b03c-0451269b1679"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    embed_layer = Embedding(vocab_size,\n",
        "                            100,weights=[embedding_matrix],\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
        "\n",
        "    x = Bidirectional(GRU(gru_len,\n",
        "                          activation='relu',\n",
        "                          dropout=dropout_p,\n",
        "                          recurrent_dropout=dropout_p,\n",
        "                          return_sequences=True))(embed_layer)\n",
        "    capsule = Capsule(\n",
        "        num_capsule=Num_capsule,\n",
        "        dim_capsule=Dim_capsule,\n",
        "        routings=Routings,\n",
        "        share_weights=True)(x)\n",
        "\n",
        "    capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    # capsule = LeakyReLU()(capsule)\n",
        "    capsule = Dense(128,activation='relu')(capsule)\n",
        "\n",
        "\n",
        "    # x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(capsule)\n",
        "    model = Model(inputs=input1, outputs=output)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy',F1Score(),keras.metrics.TruePositives(), keras.metrics.TrueNegatives(),keras.metrics.FalsePositives(), metrics.FalseNegatives()])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa0c889-d4e6-4873-83b0-a168f853e7c3",
      "metadata": {
        "id": "6aa0c889-d4e6-4873-83b0-a168f853e7c3",
        "outputId": "fd4a4e1c-da8c-40bb-f33b-0d6fc5a063c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-22 00:44:53.978852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1c:00.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 100)          236000    \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, 100, 100)          0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 100, 512)          549888    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " capsule (Capsule)           (None, 10, 16)            81920     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               20608     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 888545 (3.39 MB)\n",
            "Trainable params: 888545 (3.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac93cc88-af93-4595-9048-b717f6d21618",
      "metadata": {
        "id": "ac93cc88-af93-4595-9048-b717f6d21618",
        "outputId": "c353b7ef-2e90-4dcf-8813-008921db782b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-22 00:45:32.301610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
            "2023-11-22 00:45:45.681302: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab180289fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-22 00:45:45.681413: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
            "2023-11-22 00:45:46.019190: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-11-22 00:45:47.191821: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 67s 1s/step - loss: 0.2683 - accuracy: 0.9280 - f1_score: 0.9626 - true_positives: 862.0000 - true_negatives: 1.0000 - false_positives: 57.0000 - false_negatives: 10.0000 - val_loss: 0.2352 - val_accuracy: 0.9376 - val_f1_score: 0.9678 - val_true_positives: 872.0000 - val_true_negatives: 0.0000e+00 - val_false_positives: 58.0000 - val_false_negatives: 0.0000e+00\n",
            "Epoch 2/5\n",
            "30/30 [==============================] - 26s 856ms/step - loss: 0.2346 - accuracy: 0.9376 - f1_score: 0.9678 - true_positives: 872.0000 - true_negatives: 0.0000e+00 - false_positives: 58.0000 - false_negatives: 0.0000e+00 - val_loss: 0.2314 - val_accuracy: 0.9376 - val_f1_score: 0.9678 - val_true_positives: 872.0000 - val_true_negatives: 0.0000e+00 - val_false_positives: 58.0000 - val_false_negatives: 0.0000e+00\n",
            "Epoch 3/5\n",
            "30/30 [==============================] - 27s 891ms/step - loss: 0.2404 - accuracy: 0.9376 - f1_score: 0.9678 - true_positives: 872.0000 - true_negatives: 0.0000e+00 - false_positives: 58.0000 - false_negatives: 0.0000e+00 - val_loss: 0.2427 - val_accuracy: 0.9376 - val_f1_score: 0.9678 - val_true_positives: 872.0000 - val_true_negatives: 0.0000e+00 - val_false_positives: 58.0000 - val_false_negatives: 0.0000e+00\n",
            "Epoch 4/5\n",
            "30/30 [==============================] - 26s 881ms/step - loss: 0.2332 - accuracy: 0.9376 - f1_score: 0.9678 - true_positives: 872.0000 - true_negatives: 0.0000e+00 - false_positives: 58.0000 - false_negatives: 0.0000e+00 - val_loss: 0.2249 - val_accuracy: 0.9376 - val_f1_score: 0.9678 - val_true_positives: 872.0000 - val_true_negatives: 0.0000e+00 - val_false_positives: 58.0000 - val_false_negatives: 0.0000e+00\n",
            "Epoch 5/5\n",
            "30/30 [==============================] - 23s 777ms/step - loss: 0.2266 - accuracy: 0.9376 - f1_score: 0.9678 - true_positives: 872.0000 - true_negatives: 0.0000e+00 - false_positives: 58.0000 - false_negatives: 0.0000e+00 - val_loss: 0.2117 - val_accuracy: 0.9376 - val_f1_score: 0.9678 - val_true_positives: 872.0000 - val_true_negatives: 0.0000e+00 - val_false_positives: 58.0000 - val_false_negatives: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2aaf422e27a0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(training_padded, training_labels, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(val_padded, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da65012-d4c2-41c3-b9c4-4d0c75cb6c61",
      "metadata": {
        "id": "6da65012-d4c2-41c3-b9c4-4d0c75cb6c61",
        "outputId": "a4f51d4e-f117-40c3-c6f7-a3637afa0d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 - 1s - loss: 0.2147 - accuracy: 0.9382 - f1_score: 0.9681 - true_positives: 243.0000 - true_negatives: 0.0000e+00 - false_positives: 16.0000 - false_negatives: 0.0000e+00 - 950ms/epoch - 106ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.21467170119285583,\n",
              " 0.9382239580154419,\n",
              " 0.968127429485321,\n",
              " 243.0,\n",
              " 0.0,\n",
              " 16.0,\n",
              " 0.0]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(testing_padded, testing_labels, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4342c2ba-d7c7-4522-8f46-94d9cf4b3315",
      "metadata": {
        "id": "4342c2ba-d7c7-4522-8f46-94d9cf4b3315"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [.conda-transformers_env]",
      "language": "python",
      "name": "conda-env-.conda-transformers_env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}